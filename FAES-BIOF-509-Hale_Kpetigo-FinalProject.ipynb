{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAES BIOF-509 Final Project\n",
    "Author: Hale Kpetigo / hale.kpetigo@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import umap ###install with \"pip install umap-learn\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns ### install using pip install seaborn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Class\n",
    "# \n",
    "\n",
    "class BIOF509Unsupervised:\n",
    "    \n",
    "    # My Variables\n",
    "    #pd_data = []\n",
    "    models = []\n",
    "    labels = []\n",
    "    reduced_pd_data = []\n",
    "    scaled_data = []\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, file):\n",
    "        # Call Data Loader\n",
    "        self.load_data(file)\n",
    "        self.prep_data()\n",
    "        self.scale_data()\n",
    "        #self.reduce_data()\n",
    "\n",
    "    # Load dataset for analysis\n",
    "    def load_data(self, file):\n",
    "        self.pd_data = pd.read_csv(file)\n",
    "        #make a copy of the raw data for future use\n",
    "        self.raw_data = self.pd_data.copy(deep=True)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.pd_data\n",
    "    \n",
    "    # Prepare data - Seperate target from data\n",
    "    def prep_data(self):\n",
    "        self.labels = self.pd_data['target']\n",
    "        del self.pd_data[\"target\"]\n",
    "    \n",
    "    # helper to display target and head of the data\n",
    "    def print_data(self):\n",
    "        display(self.raw_data.head())\n",
    "        display(self.pd_data.head())\n",
    "        display(self.labels)\n",
    "    \n",
    "    # Pair plot analysis to display relationships between features in the dataset\n",
    "    def plot_data(self):\n",
    "        sns_plot = sns.pairplot(self.raw_data, hue='target')\n",
    "        #sns.pairplot(self.raw_data, vars=self.raw_data.columns[:-1],hue='target')\\\n",
    "        #save plot to disk for report\n",
    "        sns_plot.savefig(\"pairplot.png\")\n",
    "    \n",
    "    def data_analysis(self):\n",
    "        self.print_data()\n",
    "        self.plot_data()\n",
    "        \n",
    "    def plot_clusters(self):\n",
    "        #model = AgglomerativeClustering(n_clusters=3)\n",
    "        #self.cluster_labels = model.fit_predict(self.scaled_data)\n",
    "        self.reduce_data()\n",
    "        sns.scatterplot(x='UMAP Dim. #1', y='UMAP Dim. #2',\n",
    "            hue='cluster_labels', data=self.reduced_pd_data)\n",
    "    \n",
    "    def get_cluster_labels(self):\n",
    "        model = AgglomerativeClustering(n_clusters=3)\n",
    "        cluster_labels = model.fit_predict(self.scaled_data)\n",
    "        return cluster_labels\n",
    "    \n",
    "    # Scale Data\n",
    "    def scale_data(self):\n",
    "        data_for_scaling = self.pd_data.values\n",
    "        # scaler = MinMaxScaler()\n",
    "        self.scaled_data = self.scaler.fit_transform(data_for_scaling)\n",
    "        \n",
    "        #for debuging purposes\n",
    "        #print(self.scaled_data[0:5])\n",
    "    \n",
    "    # Recude Data\n",
    "    def reduce_data(self):       \n",
    "        data_reducer = umap.UMAP()\n",
    "        \n",
    "        umap_data = data_reducer.fit_transform(self.scaled_data)\n",
    "\n",
    "        scaled_umap_data = self.scaler.fit_transform(umap_data)\n",
    "\n",
    "        self.reduced_pd_data = pd.DataFrame(scaled_umap_data,columns=['UMAP Dim. #1', 'UMAP Dim. #2'])\n",
    "\n",
    "        self.reduced_pd_data['cluster_labels'] = self.get_cluster_labels()\n",
    "        \n",
    "        #for debuging purposes\n",
    "        #print(self.reduced_pd_data)\n",
    "    \n",
    "    def plotKmean_elbow(self):\n",
    "        ks = list(range(1, 10))\n",
    "        kmeans_models = []\n",
    "        inertias = []\n",
    "\n",
    "        for k in ks:\n",
    "            model = KMeans(n_clusters=k, random_state=509).fit(self.scaled_data)\n",
    "            kmeans_models.append(model)\n",
    "            inertias.append(model.inertia_)\n",
    "        \n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.plot(ks, inertias, '-x')\n",
    "        ax1.set_title('KMean Elbow')\n",
    "        ax1.set_ylabel('inertia')\n",
    "        ax1.set_xlabel('k')\n",
    "        #ax1.show()\n",
    "\n",
    "    def predict(self, _data, eps=[0.1*i for i in range(1,20)], min_samples=list(range(1,11))):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        _models = []\n",
    "        _labels = []\n",
    "        for e in eps:\n",
    "            for ms in min_samples:\n",
    "                model = DBSCAN(eps=e, min_samples=ms).fit(_data)\n",
    "                _models.append(model)\n",
    "                _labels.append(model.fit_predict(_data))\n",
    "        return _models, _labels\n",
    "    \n",
    "    def accuracy(self, target, _models, _labels):\n",
    "        score = []\n",
    "        for pred in _labels:\n",
    "            score.append(self.max_score(pred, target))\n",
    "        \n",
    "        max_ind = np.argmax(score)\n",
    "        best_model = _models[max_ind]\n",
    "        print('Max accuracy {} with model {}'.format(score[max_ind], best_model))\n",
    "        return best_model\n",
    "    \n",
    "    def max_score(self, pred, target):\n",
    "        permutations = [[0, 1], [1, 0]]\n",
    "        scores = []\n",
    "        for perm in permutations:\n",
    "            n_right = 0\n",
    "            for label, t in zip([0, 1], perm):\n",
    "                n_right += sum((pred == label)*(target == t)*1)\n",
    "            scores.append(n_right/len(pred))\n",
    "        return max(scores)\n",
    "    \n",
    "    def plot_dendrogram(self,model, **kwargs):\n",
    "        # from: https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "        # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "        # create the counts of samples under each node\n",
    "        counts = np.zeros(model.children_.shape[0])\n",
    "        n_samples = len(model.labels_)\n",
    "        for i, merge in enumerate(model.children_):\n",
    "            current_count = 0\n",
    "            for child_idx in merge:\n",
    "                if child_idx < n_samples:\n",
    "                    current_count += 1  # leaf node\n",
    "                else:\n",
    "                    current_count += counts[child_idx - n_samples]\n",
    "            counts[i] = current_count\n",
    "\n",
    "        linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                          counts]).astype(float)\n",
    "\n",
    "        # Plot the corresponding dendrogram\n",
    "        dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "    def BIOF509Kmeans(self):\n",
    "        self.plot_clusters()\n",
    "        self.plotKmean_elbow()\n",
    "    \n",
    "    def BIOF509Density(self):\n",
    "        mdls, lbls = self.predict(self.scaled_data)\n",
    "        mytarget = self.labels\n",
    "        np_lbl = mytarget.array #convert to np\n",
    "        \n",
    "        best_model = self.accuracy(np_lbl, mdls, lbls)\n",
    "        \n",
    "        # Try without age and sex features\n",
    "        mdls, lbls = self.predict(self.scaled_data[:, 2:14])\n",
    "        best_model_reduced = self.accuracy(np_lbl, mdls, lbls)\n",
    "        \n",
    "    def BIOF509Hierarchical(self):\n",
    "        model = AgglomerativeClustering(distance_threshold=2, n_clusters=None)\n",
    "        labels = model.fit_predict(self.scaled_data)\n",
    "        self.plot_dendrogram(model, truncate_mode='level', p=2)\n",
    "        plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        plt.show()\n",
    "        self.plotHierarchy_elbow()\n",
    "    \n",
    "    def plotHierarchy_elbow(self):\n",
    "        # vary parameters\n",
    "        thresh = list(range(10))\n",
    "        agg_models = []\n",
    "        n_clusters = []\n",
    "\n",
    "        for t in thresh:\n",
    "            model = AgglomerativeClustering(distance_threshold=t, n_clusters=None).fit(self.scaled_data)\n",
    "            agg_models.append(model)\n",
    "            n_clusters.append(model.n_clusters_)\n",
    "            \n",
    "        fig2, ax2 = plt.subplots()    \n",
    "        ax2.plot(thresh, n_clusters, '-x')\n",
    "        display(n_clusters)\n",
    "        \n",
    "        # plotting with agglomerative model labels\n",
    "        # When non continuous values value features are incluter the plot is linear as opposed to being globular\n",
    "        fig3, ax3 = plt.subplots()\n",
    "        # age:0 vs Max: heart rate:7 -> Globular\n",
    "        # age:0 vs sex: 1 -> linear\n",
    "        ax3.scatter(self.scaled_data[:,0], self.scaled_data[:,7], c=agg_models[2].labels_.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy 0.5478547854785478 with model DBSCAN(eps=1.1, min_samples=1)\n",
      "Max accuracy 0.6039603960396039 with model DBSCAN(eps=0.8, min_samples=6)\n"
     ]
    }
   ],
   "source": [
    "FinalProject = BIOF509Unsupervised(\"heart 2.csv\")\n",
    "# Second dataset used for comparision\n",
    "# FinalProject = BIOF509Unsupervised(\"heart.csv\")\n",
    "#FinalProject.data_analysis()\n",
    "\n",
    "# Use one alogrithm at a time\n",
    "#FinalProject.BIOF509Kmeans()\n",
    "FinalProject.BIOF509Density()\n",
    "#FinalProject.BIOF509Hierarchical()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF509Supervised:\n",
    "    \n",
    "    \"\"\"My Variables\"\"\"\n",
    "    pd_data = []\n",
    "    labels = []\n",
    "    scaled_data = []\n",
    "    reduced_pd_data = []\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, file):\n",
    "        # Call Data Loader\n",
    "        self.load_data(file)\n",
    "        self.prep_data()\n",
    "        \n",
    "    # Load data set for analysis\n",
    "    def load_data(self, file):\n",
    "        self.pd_data = pd.read_csv(file)\n",
    "    \n",
    "    def prep_data(self):\n",
    "        self.labels = self.pd_data['target']\n",
    "        del self.pd_data[\"target\"]\n",
    "        \n",
    "    def BIOF509SVM(self):\n",
    "        svm_model = SVC(kernel='linear')\n",
    "        skf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "        all_feature_importances = np.zeros((len(set(self.labels)),len(self.pd_data.columns)))\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        accuracy = 0\n",
    "\n",
    "        for train_index, test_index in skf.split(self.pd_data.values, self.labels):\n",
    "\n",
    "                ''' training dataset '''\n",
    "                X_train, y_train = self.pd_data.values[train_index], self.labels[train_index]\n",
    "\n",
    "\n",
    "                ''' testing dataset '''\n",
    "                X_test, y_test = self.pd_data.values[test_index], self.labels[test_index]\n",
    "\n",
    "\n",
    "                ''' fit the model to the data - this is where the algorithm learns the patterns and finds\n",
    "                the hyperplane'''\n",
    "                svm_model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "                ''' \n",
    "                The coef values tell us which features were most important in finding the SVM hyperplanes\n",
    "                We can only do this for a linear SVM, which does not add on extra dimensions. \n",
    "\n",
    "                In many cases, decision tree methods work better for interpretation\n",
    "                '''\n",
    "                all_feature_importances += svm_model.coef_\n",
    "\n",
    "\n",
    "                ''' now, we will predict the labels'''\n",
    "                predictions = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "                ''' \n",
    "                Use the extend function to add the new predictions onto the list of predictions\n",
    "                '''\n",
    "                all_predictions.extend(predictions)\n",
    "\n",
    "                ''' Use extend function to add the new labels onto the list of labels'''\n",
    "                all_labels.extend(y_test)\n",
    "\n",
    "                ''' add the new accuracy score to the other scores - we will average them at the end'''\n",
    "                accuracy += accuracy_score(predictions,y_test)\n",
    "\n",
    "        print(\"PREDICTION ACCURACY\")\n",
    "        print(accuracy/5)\n",
    "        print()\n",
    "        print('ALL LABELS')\n",
    "        print(all_labels)\n",
    "        print()\n",
    "        print('ALL PREDICTIONS')\n",
    "        print(all_predictions)\n",
    "        \n",
    "        # Plot Confusion Matrix\n",
    "        ''' here, I input the labels and predictions into the Scikit learn confusion_matrix function and they\n",
    "        will generate a confusion matrix for me'''\n",
    "        cm = confusion_matrix(all_labels,all_predictions,normalize= 'true')\n",
    "\n",
    "        ''' turn it into a dataframe so seaborn will label the graph using the columns/indicies of the df'''\n",
    "        df_cm = pd.DataFrame(cm,index=[0,1],columns=[0,1])\n",
    "\n",
    "        ''' graph using seaborn heatmap function - annot=True so labels will appear'''\n",
    "        sns.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        # Plot Inportance of features\n",
    "        average_feature_importances = []\n",
    "\n",
    "        for i in range(all_feature_importances.shape[1]):\n",
    "            average_value = 0\n",
    "            for array in all_feature_importances:\n",
    "                average_value += abs(array[i])\n",
    "            average_feature_importances.append(average_value/3) \n",
    "            '''3 instead of 5'''\n",
    "\n",
    "        print(average_feature_importances)    \n",
    "\n",
    "        ''' make the figure a little bigger than default'''\n",
    "        plt.figure(figsize = (15,10))\n",
    "\n",
    "        ''' use matplotlib's bar graph function to graph feature importances\n",
    "        X axis = feature name\n",
    "        y axis = feature importance values\n",
    "        '''\n",
    "        plt.bar(list(self.pd_data.columns),average_feature_importances, color ='blue',  width = 0.4) \n",
    "        plt.title('Feature Importances')\n",
    "        \n",
    "    def BIOFDecisionTree(self):\n",
    "        \n",
    "        print('**Decision Tree')\n",
    "        \n",
    "        # Setting data attributes\n",
    "        X = self.pd_data\n",
    "        # Setting target\n",
    "        y = self.labels\n",
    "\n",
    "        print('Dataset size:', X.shape[0])\n",
    "\n",
    "        # Using the train_test_split to create train and test sets.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 47, test_size = 0.33)\n",
    "        \n",
    "        # Importing the Decision tree classifier from the sklearn library.\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        clf = DecisionTreeClassifier(criterion = 'entropy')\n",
    "        \n",
    "        # Training the decision tree classifier. \n",
    "        clf.fit(X_train, y_train)\n",
    "            \n",
    "        # Predicting labels on the test set.\n",
    "        y_pred =  clf.predict(X_test)\n",
    "        \n",
    "        print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
    "        print('Accuracy Score on test data: ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "        clf = DecisionTreeClassifier(criterion='entropy', min_samples_split=50)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
    "        print('Accuracy Score on the test data: ', accuracy_score(y_true=y_test, y_pred=clf.predict(X_test)))\n",
    "        print('***')\n",
    "        \n",
    "    def BIOFBayesian(self):\n",
    "        \n",
    "        print('**GaussianNB')\n",
    "        # Setting data attributes\n",
    "        X = self.pd_data\n",
    "        # Setting target\n",
    "        y = self.labels\n",
    "        \n",
    "        print('Dataset size:', X.shape[0])\n",
    "\n",
    "        # Using the train_test_split to create train and test sets.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 47, test_size = 0.33)\n",
    "        \n",
    "        #x_train, y_train, x_test, y_test = datasets.breastcancer_cont(replication=1)\n",
    "        #train_data, test_data = datasets.breastcancer_cont_discomll(replication=1)\n",
    "\n",
    "        clf = GaussianNB()\n",
    "        probs_log1 = clf.fit(X_train, y_train).predict_proba(X_test)\n",
    "        \n",
    "        y_pred =  clf.predict(X_test)\n",
    "        \n",
    "        #res = clf.score(X_test, y_test)\n",
    "        #display(res)\n",
    "        \n",
    "        print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
    "        print('Accuracy Score on test data: ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**GaussianNB\n",
      "Dataset size: 303\n",
      "Accuracy Score on train data:  0.8522167487684729\n",
      "Accuracy Score on test data:  0.8\n"
     ]
    }
   ],
   "source": [
    "FinalSupProject = BIOF509Supervised(\"heart 2.csv\")\n",
    "\n",
    "#FinalSupProject.BIOF509SVM()\n",
    "\n",
    "#FinalSupProject.BIOFDecisionTree()\n",
    "\n",
    "FinalSupProject.BIOFBayesian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
